*	复习:卷积的过程	
*	![avatar](https://img-blog.csdn.net/20161021135241205)
*	空洞卷积(dilation conv)
*	一般情况下图片输入到网络中会对图像做卷积再池化,pooling的操作在降低图像尺寸的同时增大感受野，但由于图像分割预测的是pixel-wise(逐像素)的输出，所以将pooling后较小的图像上采样到原始图像的尺寸进行预测，一般需要使用反卷积的操作，而池化层一般使用max pooling，max pooling可以使得每个pixel的感受✌野增大
*	由于在先减小再增大的过程中会有一些信息损失，为了减少由pooling带来的信息损失，所以提出了空洞卷积，空洞卷积可以不通过pooling来增大感受野,空洞卷积的关键参数raterate.
	*	从原图的角度看空洞卷积的过程：在原图采样，当raterate为1时表示原图不失任何信息的采样，此时的卷积对应标准的卷积操作，而当raterate=2时在原图上每个采样点之间间隔1，然后将采样结果与kernel卷积，这样就增大了图像的感受野，而没有信息损失。
	*	从kernel的角度去看待空洞卷积，相当于在相邻点之间插入raterate-1个0,然后再将扩大的kernel和原图作卷积。
	*	扩展资料：在VGG网络中就证明了使用小卷积核叠加来取代大卷积核可以起到减少参数同时达到大卷积核同样大小感受野的功效。但是通过叠加小卷积核来扩大感受野只能线性增长，而空洞卷积可以以指数级增长感受野。
*	反卷积(decov)
	*	如下图其中蓝色为原图，白色为pading，以此使得图片size增大，也可以在原像素之间添加padding,即反卷积
	*	![avatar](https://img-blog.csdn.net/20161021141659634)
	*	反卷积
	*	![avatar](https://img-blog.csdn.net/20161021154222794)
	*	主要用于增大图像的尺寸，是上采样(upsampling)的一种（空洞卷积没有使用上采样的情况下增大感受野可以不改变图像的大小)
	*	反卷积的简单理解就是在输入特征矩阵中插入空白点，再进行卷积这样卷积后输出的特征矩阵就变大了
	*	对于同一卷积核，当步长>1时，相当于在卷积的同时下采样，卷积后的图像变小了(也可以增大感受野，单数输出图像变小了)，步长<1时相当于对原图作上采样再卷积，这样结果图就变大了。
*	感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上每个像素点在原始图像上映射的区域大小，这里的原始图像是指网络的输入图像，是经过预处理（如resize，warp，crop）后的图像。
	*	感受野的计算
*	FC(Full connected)全连接层
	*	
*	BP
*	DepthWise Convolution
*	Group Convolution
	* ![avatar](https://s2.ax1x.com/2019/01/08/FLPc1x.png)
	*	分组卷积是对输入的FeatureMap进行卷积然后分别卷积，最早用于多个GPU的分别训练
*	Global DepthWise Convolution	
*	ReLU: Rectifiled Linear Uint 线性修正函数
	*	ReLU(x)=max(0,x)
	*	Relu可以实现单侧抑制（即把一部分神经元置0），能够稀疏模型， Sigmoid 型活tanh激活函数会导致一个非稀疏的神经网络，而Relu大约 50% 的神经元会处于激活状态，具有很好的稀疏性
	*	tanh 激活函数的定义
		*	$f(x)=\frac{e^x- e^{-x}}{e^x+e^{-x}}$
	*	Relu函数大于0的线性部分梯度始终为1，具有 宽兴奋边界的特性 （即兴奋程度可以非常高），不会发生神经网络的梯度消失问题， 能够加速梯度下降的收敛速度。而tanh和sigmoid在离0点近的时候梯度大，在远离0点的时候梯度小，容易出现梯度消失
	*	缺点:
		*	ReLU 函数不是在0周围， 相当于给后一层的神经网络引入偏置偏移，会影响梯度下降的效率。另外，在训练时， 如果参数在一次不恰当的更新后， 某个 ReLU 神经元输出为0，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活， 这种现象称为死亡 ReLU 问题 （Dying ReLU Problem）
			*	出现Dying ReLU 可能是因为学习率太大，导致w更新巨大，使得输入数据在经过这个神经元的时候，输出值小于0，从而经过激活函数的时候为0，从此不再更新。所以relu为激活函数，学习率不能太大
*	Leaky ReLU(带泄露的Relu)
	*	Leaky ReLU(X)=x if x>0 else$\gamma x,其中\gamma是一个很小的参数$，这样可以避免因为神经元输出为负数而导致的后续永远不被激活（死亡)	
*	PReLU(Parametric ReLU， PReLU，即带参数的 ReLU)
	*	将Leaky Relu中的$\gamma$变成可学习的参数，即每一个神经元可以有不同的$\gamma$
*	ELU (指数线性单元)
	*	ELU(x)=x if x>0 else $\gamma(\exp(x)-1)$,其中的$\gamma>=0$是一个超参（学习前预设的参数)
*	SoftPlus
	*	SoltPlus 可以看作是ReLU的平滑版本
		*	$SoftPlus(x)=\log(1+\exp(x))$ (运算量大)
		*	Softplus 函数其导数刚好是 Logistic 函数(sigmoid),Softplus 函数虽然也具有单侧抑制、 宽兴奋边界的特性， 却没有稀疏激活性，不会稀疏模型。


*	Deopout Layer
	*	Deopout 是指在模型训练时随机让某些神经单元的权重停止本次更新，等下次训练时就有可能更新，对于一层中每个神经单元来说，它们更新的概率是相同的。Dropout说的简单一点就是我们让在前向传导的时候，让某个神经元的激活值以一定的概率p，让其停止工作
	*	主要作用：防止模型过拟合
*	Normalization
	*	Batch Normalization 
		*	在神经网络的训练过程中，我们一般会将输入样本特征进行归一化处理，使数据变为均值为0，标准差为1的分布或者范围在0~1的分布。因为当我们没有将数据进行归一化的话，由于样本特征分布较散，可能会导致神经网络学习速度缓慢甚至难以学习。
		*	BN的提出是为了解决 Internal Covariate Shift 问题的。
			*	网络一旦训练起来，参数就要发生更新，出了输入层的数据外，其它层的数据分布是一直发生变化的，因为在训练的时候，网络参数的变化就会导致后面输入数据的分布变化，比如第二层输入，是由输入数据和第一层参数得到的，而第一层的参数随着训练一直变化，势必会引起第二层输入分布的改变，把这种改变称之为 Internal Covariate Shift。
			*	BN的本质是对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题（越靠近非线性边缘梯度变化越小)。
			*	BN 使得每一层的梯度变化都处于较大的状态，因此调参效率较高收敛较快。
		*	BN的流程
			*	输入为N*C*W*H 即N个样本，每个样本的通道数为C
			*	训练阶段
				*	就是把第1个样本的第1个通道，加上第2个样本第1个通道 ...... 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值。对所有通道都施加一遍这个操作，就得到了所有通道的均值和方差，再将每个pixel对应的值减去均值，除以方差，就得到了规范化的结果。在此基础上，BN还增加了两个可训练的参数$\gamma,\beta$。
				*	对每个通道上求数据均值,求数据方差,数据进行标准化,训练参数γ，β,输出y通过γ与β的线性变换得到新的值
					*	$\gamma,\beta$的作用：没有这两个学习参数的情况下，直接使用归一化强行将上一层的输出结果进行均值为0标准差为1的归一化会将上一层学习到的分布给破坏掉，因此引入两个可学习的参数$\gamma,\beta$用于变换重构，让网络可以学习恢复出原始网络所要学习的特征分布
				*	均值：$$\mu(x)=\frac{1}{NHW}\sum_{n=1}^{N}\sum_{h=1}^{H}\sum_{w=1}^{W}x_{nchw}$$
				*	方差: $$\sigma_c(x)=\sqrt{\frac{1}{NHW}\sum_{n=1}^{N}\sum_{h=1}^{H}\sum_{w=1}^{W}(x_{nchw}-\mu(x))^2+\epsilon}$$
				*	加入可训练参数后的公式:$$x_{out}=\gamma\frac{x-\mu(x)}{\sigma(x)}+\beta$$
			*	预测阶段
				*	预测阶段数据很少,$\mu和\sigma的计算和训练中的相比有很大的偏差$
				*	对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch σB的无偏估计
		*	BN并不能阻止过拟合，在某些情况下可以抑制，BN的本意是为了加速训练和收敛过程；在训练中，BN的使用使得一个mini-batch中的所有样本都被关联在了一起，一个batch数据中每张图片对应的输出都受到一个batch所有数据影响,这样相当于一个间接的数据增强,达到防止过拟合作用.		
  	*   Local Response Normalization
      	*   说到为什么要使用LRN就不得不提到神经生物学中的一个概念叫做 lateral inhibition（横向抑制），简单来讲就是兴奋的神经细胞抑制周围神经细胞的能力。应用到深度神经网络中，这种横向抑制的目的是进行局部对比度增强，以便将局部特征在下一层得到表达,LRN是一个非训练层，即该层中不存在可训练的参数。
      	*   

*	激活函数
	*	激活函数（activation functions）的目标是，将神经网络非线性化。激活函数是连续的（continuous），且可导的（differential），即激活函数是向网络中加入非线性从而解决线性	不能解决的问题。侧面表明激活函数一定是非线性的。
	*	![avatar](https://upload-images.jianshu.io/upload_images/749674-cdc2da4f770158ca.png?imageMogr2/auto-orient/strip|imageView2/2)
	*	https://www.cnblogs.com/further-further-further/p/10430073.html

*	卷积运算转换成矩阵运算
